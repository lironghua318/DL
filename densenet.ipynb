from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf

slim = tf.contrib.slim

num_classes=200
def trunc_normal(stddev): return tf.truncated_normal_initializer(stddev=stddev)


def bn_act_conv_drp(current, num_outputs, kernel_size, scope='block'):
    current = slim.batch_norm(current, scope=scope + '_bn')
    current = tf.nn.relu(current)
    current = slim.conv2d(current, num_outputs, kernel_size, scope=scope + '_conv')
    current = slim.dropout(current, scope=scope + '_dropout')
    return current


def block(net, layers, growth, scope='block'):
    for idx in range(layers):
        bottleneck = bn_act_conv_drp(net, 4 * growth, [1, 1],
                                     scope=scope + '_conv1x1' + str(idx))
        tmp = bn_act_conv_drp(bottleneck, growth, [3, 3],
                              scope=scope + '_conv3x3' + str(idx))
        net = tf.concat(axis=3, values=[net, tmp])
    return net


def densenet(inputs, num_classes=200, is_training=True,
             dropout_keep_prob=0.8,
             scope='densenet'):

    growth = 12
    compression_rate = 0.5

    def reduce_dim(input_feature):
        return int(int(input_feature.shape[-1]) * compression_rate)

    end_points = {}

    with tf.variable_scope(scope, 'DenseNet', [inputs, num_classes]):
        with slim.arg_scope(bn_drp_scope(is_training=is_training,
                                         keep_prob=dropout_keep_prob)) as ssc:
            pass
            scope = 'conv1'
            net = slim.conv2d(inputs,2*growth,[7,7],scope=scope)
            end_points[scope] = net
            
            #output 320*320*304
            scope = 'block1'
            net = block(net,12,growth,scope=scope)
            end_points[scope] = net
            
            #output 320*320*152
            scope = 'compress1'
            net = bn_act_conv_drp(net,reduce_dim(net),[1,1],scope=scope)
            end_points[scope] = net
            
            #output 160*160*152
            scope = 'avgpool1'
            net = slim.avg_pool2d(net,[2,2],stride = 2,scope=scope)
            end_points[scope] = net
            
            #output 160*160*440
            scope = 'block2'
            net = block(net,12,growth,scope=scope)
            end_points[scope]=net
            
            #output 160*160*220
            scope ='compress2'
            net = bn_act_conv_drp(net,reduce_dim(net),[1,1],scope=scope)
            end_points[scope]=net
            
            #output 80*80*220
            scope = 'avgpool2'
            net = slim.avg_pool2d(net,[2,2],stride=2,scope=scope)
            end_points[scope]=net
            
            #output 80*80*508
            scope ='block3'
            net = block(net,12,growth,scope=scope)
            end_points[scope]=net
            
            #output 80*80*508
            scope ='last_batch_nom_act'
            net = slim.batch_norm(net,scope=scope)
            net = tf.nn.relu(net)
            end_points[scope]=net
            
            #output 1*1*508
            #net = slim.avg_pool2d(net,net.shape[1:3])
            #output 1*1*200
            #net = slim.flatten(net, scope='PreLogitsFlatten')          
            biases_initializer = tf.constant_initializer(0.1)
            net = slim.conv2d(net,num_classes,[1,1],biases_initializer=biases_initializer)
            net = slim.flatten(net, scope='PreLogitsFlatten')
            logits = slim.fully_connected(net, num_classes, activation_fn=None,
                                        scope='Logits')            
            #logits = tf.squeeze(net) 
            end_points['Logits'] = logits
            end_points['Predictions'] = tf.nn.softmax(logits, name='Predictions')

    return logits, end_points


def bn_drp_scope(is_training=True, keep_prob=0.8):
    keep_prob = keep_prob if is_training else 1
    with slim.arg_scope(
        [slim.batch_norm],
            scale=True, is_training=is_training, updates_collections=None):
        with slim.arg_scope(
            [slim.dropout],
                is_training=is_training, keep_prob=keep_prob) as bsc:
            return bsc


def densenet_arg_scope(weight_decay=0.0004):

    with slim.arg_scope(
        [slim.conv2d],
        weights_initializer=tf.contrib.layers.variance_scaling_initializer(
            factor=2.0, mode='FAN_IN', uniform=False),
        activation_fn=None, biases_initializer=None, padding='same',
            stride=1) as sc:
        return sc


densenet.default_image_size = 112
